import pandas as pd
from sodapy import Socrata
### Reference: Overall CO2 Emission Trends in Globe
Reference = pd.read_csv("annual-co2-emissions-per-country.csv")
Reference.head()


Reference['Year'] = pd.to_datetime(Reference['Year'], format='%Y')

mean_co2_emissions = Reference.groupby("Year")['Annual CO₂ emissions'].mean()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(mean_co2_emissions.index, mean_co2_emissions.values, label='Mean CO2 Emissions')
plt.xlabel('Year')
plt.ylabel('Mean CO2 Emission')
plt.title('Mean CO2 Emissions')
plt.legend()
plt.grid(True)
plt.show()
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt 
import seaborn as sns 

client = Socrata("data.cityofnewyork.us", None)
results = client.get("7x5e-2fxh", limit=2000)
results_df = pd.DataFrame.from_records(results)
results_df.head()
results_df.shape
### Data Preprocessing
# Convert Object to Numeric Values
text_numeric_columns = results_df.select_dtypes(include=['object']).columns
text_numeric_columns

for col in text_numeric_columns:
    results_df[col] = pd.to_numeric(results_df[col], errors='coerce')
    
    
# Use dropna() with the thresh parameter to drop columns

threshold = len(results_df) * 0.9
df_cleaned = results_df.dropna(axis=1, thresh=threshold)
df_cleaned.shape[1]

df_cleaned.head()
# Cleaning: Inf or NaN Values

inf_columns = np.isinf(df_cleaned).any()
count_true = inf_columns.sum()
count_true


# Check for missing values (NaN)
nan_columns = np.isnan(df_cleaned).any()
count_true = nan_columns.sum()
count_true 

# Columns still having NaN
columns_with_nan = df_cleaned.columns[df_cleaned.isna().any()]

df_cleaned.fillna(df_cleaned.median(), inplace=True)

df_cleaned.isna().sum()

### Traditional Statistical Model
selected_columns = ['number_of_buildings', 'year_built', 'occupancy', 'property_gfa_self_reported', 'weather_normalized_site', 
                    'site_energy_use_kbtu', 'net_emissions_metric_tons']

df_cleaned = df_cleaned[selected_columns]

df_cleaned.head()

new_column_names = {
    'number_of_buildings': 'Numer of Buildings',
    'year_built': 'Year Built',
    "occupancy": "Occupancy",
    'property_gfa_self_reported': 'Property Gross Floor Area', 
    'weather_normalized_site': 'Weather Normalized Site (EUI)', 
    'site_energy_use_kbtu': 'Site Energy Use (Kbtu)', 
    'net_emissions_metric_tons': 'Net Carbon Emissions (Metric tons)'
}

# Rename columns using the rename method
df_cleaned.rename(columns = new_column_names, inplace=True)

df_cleaned.head()

X = df_cleaned.loc[:, df_cleaned.columns != 'Net Carbon Emissions (Metric tons)']

y = df_cleaned['Net Carbon Emissions (Metric tons)'] 

# Add a constant column for the intercept
X = sm.add_constant(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = sm.OLS(y_train, X_train)

# Fit the model to the training data
results = model.fit()

# Print the summary statistics, including p-values
print(results.summary())

### Sort by p-value 
coefficients = results.params
p_values = results.pvalues
conf_int = results.conf_int()

# Step 3: Rank Variables by p-values (Statistical Significance)
# Create a DataFrame to store variable statistics
variable_stats = pd.DataFrame({'Coefficient': coefficients, 'P-value': p_values})

# Sort variables by ascending p-values
sorted_stats = variable_stats.sort_values(by='P-value')

# Step 4: Select a Subset of Variables
# You can choose to keep variables with a specific significance threshold (e.g., p < 0.05)
significant_variables = sorted_stats[sorted_stats['P-value'] < 0.05]

print(sorted_stats[:20])

### Regression Plot of Each Independent Variable vs. Carbon Emission
for column in df_cleaned.columns:
    if column != 'Net Carbon Emissions (Metric tons)':
        sns.regplot(data=df_cleaned, x=column, y='Net Carbon Emissions (Metric tons)')
        plt.title(f'{column} vs. Net Carbon Emissions')
        plt.show()
#### Correlation between Independent Variables 

corr_matrix = df_cleaned.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(30, 10))  # Set the figure size (adjust as needed)
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

# Set the plot title
plt.title('Correlation Matrix Heatmap')

# Show the plot
plt.show()
#### ML Model 1) KNN
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score

from sklearn.preprocessing import MinMaxScaler 
from sklearn import preprocessing


X = df_cleaned.drop(columns=['Net Carbon Emissions (Metric tons)'])
   
y = df_cleaned['Net Carbon Emissions (Metric tons)'] 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = preprocessing.StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)

X_test_scaled = scaler.transform(X_test)

knn = KNeighborsRegressor()
knn.fit(X_train_scaled, y_train)
knn_train_score = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
knn_train_score = (-knn_train_score).mean()

knn_test_score = cross_val_score(knn, X_test_scaled, y_test, cv=5, scoring='neg_mean_squared_error')
knn_test_score = (-knn_test_score).mean()


print(f"Training Set'S MSE: {knn_train_score}")
print(f"Test Set'S MSE: {knn_test_score}")


print("Training-set score: {:.3f}".format(knn.score(X_train_scaled, y_train)))
print("Test-set score: {:.3f}".format(knn.score(X_test_scaled, y_test)))

### ML Model 2) Linear Regression Model 
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(X_train_scaled, y_train)
OLS_train_score = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
OLS_train_score = (-OLS_train_score).mean()


OLS_test_score = cross_val_score(lr, X_test_scaled, y_test, cv=5, scoring='neg_mean_squared_error')
OLS_test_score = (-OLS_test_score).mean()


print(f"Training Set'S MSE: {OLS_train_score}")
print(f"Test Set'S MSE: {OLS_test_score}")

print("Training-set score: {:.3f}".format(lr.score(X_train_scaled, y_train)))
print("Test-set score: {:.3f}".format(lr.score(X_test_scaled, y_test)))
#### ML Model 3) Lasso 
from sklearn.linear_model import Lasso

lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train_scaled, y_train)
lasso001_train_score = cross_val_score(lasso001, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
lasso001_train_score = (-lasso001_train_score).mean()


lasso001_test_score = cross_val_score(lasso001, X_test_scaled, y_test, cv=5, scoring='neg_mean_squared_error')
lasso001_test_score = (-lasso001_test_score).mean()


print(f"Training Set'S MSE: {lasso001_train_score}")
print(f"Test Set'S MSE: {lasso001_test_score}")
print("Number of features used: {}".format(np.sum(lasso001.coef_ != 0)))

print("Training-set score: {:.3f}".format(lasso001.score(X_train_scaled, y_train)))
print("Test-set score: {:.3f}".format(lasso001.score(X_test_scaled, y_test)))

### R^2
from sklearn.metrics import confusion_matrix

y_pred = knn.predict(X_test_scaled)

# Calculate the R-squared score
r_squared = r2_score(y_test, y_pred)

print(f"R-squared (R²) Score: {r_squared:.2f}")

from sklearn.metrics import confusion_matrix

y_pred = lr.predict(X_test_scaled)

# Calculate the R-squared score
r_squared = r2_score(y_test, y_pred)

print(f"R-squared (R²) Score: {r_squared:.2f}")

from sklearn.metrics import confusion_matrix

y_pred = lasso001.predict(X_test_scaled)

# Calculate the R-squared score
r_squared = r2_score(y_test, y_pred)

print(f"R-squared (R²) Score: {r_squared:.2f}")

### Coefficients by Importance
coefficients = lasso001.coef_   

print(f"Linear Regression Coefficients (scikit-learn): {coefficients}")


import matplotlib.pyplot as plt
feature_names = X_train.columns 

# 2nd Model) Lasso-scaled
plt.subplot(1, 2, 2)

# Create a bar plot to visualize the coefficients
plt.barh(feature_names, coefficients, color='skyblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature Name')
plt.title('Lasso Coefficients')
plt.gca().invert_yaxis()  # Invert the y-axis to display features from top to bottom